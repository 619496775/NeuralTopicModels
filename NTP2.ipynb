{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is enabled)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy\n",
    "from keras.models import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.embeddings import *\n",
    "from keras.optimizers import SGD,Adadelta,Adam\n",
    "from keras.regularizers import l2, l1l2\n",
    "from keras.constraints import unitnorm,nonneg\n",
    "from keras.layers.advanced_activations import ThresholdedReLU\n",
    "from keras import backend as K\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from keras.models import model_from_json\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import to_graph\n",
    "from keras.callbacks import ModelCheckpoint,RemoteMonitor\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import h5py\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "to_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28956, 300)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = loadmat(to_path + \"t1_termatrix.mat\", variable_names = \"target\").get(\"target\").astype(\"float32\")\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SymmetricAutoencoder(Layer):\n",
    "    '''AutoEncoder where reconstruction = reconstruction_activation(activation(x * W) * W')\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(nb_samples, input_dim)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(nb_samples, input_dim)` if output_reconstruction = True,\n",
    "        shape: `(nb_samples,output_dim)` if output_reconstruction = False\n",
    "    # Arguments\n",
    "        output_dim: int > 0.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights\n",
    "            initialization. This parameter is only relevant\n",
    "            if you don't pass a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "            The list should have 1 element, of shape `(input_dim, output_dim)`.\n",
    "        output_reconstruction: Whether, when not being trained, the output of the \n",
    "            layer should be the reconstructed input, or the hidden layer activations.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        input_dim: dimensionality of the input (integer).\n",
    "            This argument (or alternatively, the keyword argument `input_shape`)\n",
    "            is required when using this layer as the first layer in a model.\n",
    "    '''\n",
    "    input_ndim = 2\n",
    "\n",
    "    def __init__(self, output_dim, init='glorot_uniform', activation='linear',\n",
    "                 reconstruction_activation='linear', weights=None,\n",
    "                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n",
    "                 output_reconstruction=False,\n",
    "                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n",
    "        self.init = initializations.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.reconstruction_activation = activations.get(reconstruction_activation)\n",
    "        self.output_reconstruction = output_reconstruction\n",
    "        self.output_dim = output_dim\n",
    "        self.pretrain = True\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "        self.constraints = [self.W_constraint, self.b_constraint]\n",
    "\n",
    "        self.initial_weights = weights\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        self.input = K.placeholder(ndim=2)\n",
    "        super(SymmetricAutoencoder, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_dim = self.input_shape[1]\n",
    "\n",
    "        self.W = self.init((input_dim, self.output_dim))\n",
    "\n",
    "        self.params = [self.W]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.W_regularizer:\n",
    "            self.W_regularizer.set_param(self.W)\n",
    "            self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "        if self.activity_regularizer:\n",
    "            self.activity_regularizer.set_layer(self)\n",
    "            self.regularizers.append(self.activity_regularizer)\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        if self.pretrain or self.output_reconstruction: \n",
    "            return self.input_shape\n",
    "        else:\n",
    "            return (self.input_shape[0], self.output_dim)\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        if self.pretrain or self.output_reconstruction: \n",
    "            output = self.reconstruction_activation(K.dot(self.activation(K.dot(X, self.W)), K.transpose(self.W)))\n",
    "            return output            \n",
    "        else:\n",
    "            output = self.activation(K.dot(X, self.W))\n",
    "            return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'name': self.__class__.__name__,\n",
    "                  'output_dim': self.output_dim,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'reconstruction_activation': self.reconstruction_activation.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(SymmetricAutoencoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = Sequential()\n",
    "encoder.add(Embedding(\n",
    "        input_dim = term_matrix.shape[0], \n",
    "                          output_dim = 300,\n",
    "                          weights = [term_matrix], \n",
    "                           trainable = False,\n",
    "                           input_length = 1)\n",
    "    )\n",
    "encoder.add(Flatten())\n",
    "encoder.add(SymmetricAutoencoder(\n",
    "        activation = 'sigmoid',\n",
    "        reconstruction_activation = 'linear',\n",
    "        output_dim=40\n",
    "    ))\n",
    "inputs = numpy.reshape(numpy.arange(term_matrix.shape[0]), (term_matrix.shape[0], 1))\n",
    "outputs = term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "28956/28956 [==============================] - 36s - loss: 0.0145    \n",
      "Epoch 2/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0115    \n",
      "Epoch 3/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0111    \n",
      "Epoch 4/1000\n",
      "28956/28956 [==============================] - 45s - loss: 0.0110    \n",
      "Epoch 5/1000\n",
      "28956/28956 [==============================] - 42s - loss: 0.0109    \n",
      "Epoch 6/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0109    \n",
      "Epoch 7/1000\n",
      "28956/28956 [==============================] - 36s - loss: 0.0109    \n",
      "Epoch 8/1000\n",
      "28956/28956 [==============================] - 44s - loss: 0.0108    \n",
      "Epoch 9/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0108    \n",
      "Epoch 10/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0108    \n",
      "Epoch 11/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0108    \n",
      "Epoch 12/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0108    \n",
      "Epoch 13/1000\n",
      "28956/28956 [==============================] - 42s - loss: 0.0108    \n",
      "Epoch 14/1000\n",
      "28956/28956 [==============================] - 43s - loss: 0.0108    \n",
      "Epoch 15/1000\n",
      "28956/28956 [==============================] - 39s - loss: 0.0108    \n",
      "Epoch 16/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 17/1000\n",
      "28956/28956 [==============================] - 39s - loss: 0.0107    \n",
      "Epoch 18/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 19/1000\n",
      "28956/28956 [==============================] - 45s - loss: 0.0107    \n",
      "Epoch 20/1000\n",
      "28956/28956 [==============================] - 39s - loss: 0.0107    \n",
      "Epoch 21/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 22/1000\n",
      "28956/28956 [==============================] - 39s - loss: 0.0107    \n",
      "Epoch 23/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 24/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0107    \n",
      "Epoch 25/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 26/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 27/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 28/1000\n",
      "28956/28956 [==============================] - 47s - loss: 0.0107    \n",
      "Epoch 29/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0107    \n",
      "Epoch 30/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0107    \n",
      "Epoch 31/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 32/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 33/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 34/1000\n",
      "28956/28956 [==============================] - 41s - loss: 0.0107    \n",
      "Epoch 35/1000\n",
      "28956/28956 [==============================] - 47s - loss: 0.0107    \n",
      "Epoch 36/1000\n",
      "28956/28956 [==============================] - 41s - loss: 0.0107    \n",
      "Epoch 37/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 38/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 39/1000\n",
      "28956/28956 [==============================] - 44s - loss: 0.0107    \n",
      "Epoch 40/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 41/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 42/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 90/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 93/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 98/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 101/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 106/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 109/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 112/1000\n",
      "28956/28956 [==============================] - 40s - loss: 0.0107    \n",
      "Epoch 117/1000\n",
      "28956/28956 [==============================] - 40s - loss: 0.0107    \n",
      "Epoch 122/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 127/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 130/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 135/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 138/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 141/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 149/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 152/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 157/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 160/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 163/1000\n",
      "28956/28956 [==============================] - 42s - loss: 0.0107    \n",
      "Epoch 170/1000\n",
      "28956/28956 [==============================] - 39s - loss: 0.0107    \n",
      "Epoch 174/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 179/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 182/1000\n",
      "28956/28956 [==============================] - 39s - loss: 0.0107    \n",
      "Epoch 187/1000\n",
      "28956/28956 [==============================] - 42s - loss: 0.0107    \n",
      "Epoch 194/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 199/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 202/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0107    \n",
      "Epoch 207/1000\n",
      "28956/28956 [==============================] - 36s - loss: 0.0107    \n",
      "Epoch 212/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0107    \n",
      "Epoch 217/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0107    \n",
      "Epoch 222/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 227/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 232/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 235/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 238/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 243/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 246/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 257/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 260/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 265/1000\n",
      "28956/28956 [==============================] - 36s - loss: 0.0107    \n",
      "Epoch 268/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 276/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0107    \n",
      "Epoch 284/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 289/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0107    \n",
      "Epoch 300/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 303/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 306/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 309/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 312/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0107    \n",
      "Epoch 315/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 318/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 321/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 324/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 327/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 330/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 356/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 359/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 362/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 365/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 368/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0107    \n",
      "Epoch 371/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0107    \n",
      "Epoch 374/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0106    \n",
      "Epoch 397/1000\n",
      "28956/28956 [==============================] - 36s - loss: 0.0106    \n",
      "Epoch 408/1000\n",
      "28956/28956 [==============================] - 36s - loss: 0.0107    \n",
      "Epoch 413/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 454/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 457/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 460/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 463/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0106    \n",
      "Epoch 504/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0107    \n",
      "Epoch 526/1000\n",
      "28956/28956 [==============================] - 56s - loss: 0.0106    \n",
      "Epoch 530/1000\n",
      "28956/28956 [==============================] - 46s - loss: 0.0106    \n",
      "Epoch 537/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0106    \n",
      "Epoch 550/1000\n",
      "28956/28956 [==============================] - 43s - loss: 0.0106    \n",
      "Epoch 555/1000\n",
      "28956/28956 [==============================] - 40s - loss: 0.0106    \n",
      "Epoch 562/1000\n",
      "28956/28956 [==============================] - 41s - loss: 0.0106    \n",
      "Epoch 564/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0106    \n",
      "Epoch 569/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0106    \n",
      "Epoch 574/1000\n",
      "28956/28956 [==============================] - 48s - loss: 0.0106    \n",
      "Epoch 584/1000\n",
      "28956/28956 [==============================] - 44s - loss: 0.0106    \n",
      "Epoch 586/1000\n",
      "28956/28956 [==============================] - 50s - loss: 0.0106    \n",
      "Epoch 588/1000\n",
      "28956/28956 [==============================] - 57s - loss: 0.0106    \n",
      "Epoch 592/1000\n",
      "28956/28956 [==============================] - 136s - loss: 0.0106   \n",
      "Epoch 594/1000\n",
      "28956/28956 [==============================] - 114s - loss: 0.0106   \n",
      "Epoch 597/1000\n",
      "28956/28956 [==============================] - 100s - loss: 0.0106   \n",
      "Epoch 601/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0106    \n",
      "Epoch 603/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 617/1000\n",
      "28956/28956 [==============================] - 40s - loss: 0.0106    \n",
      "Epoch 622/1000\n",
      "28956/28956 [==============================] - 43s - loss: 0.0106    \n",
      "Epoch 630/1000\n",
      "28956/28956 [==============================] - 42s - loss: 0.0106    \n",
      "Epoch 640/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0106    \n",
      "Epoch 645/1000\n",
      "28956/28956 [==============================] - 43s - loss: 0.0106    \n",
      "Epoch 650/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0106    \n",
      "Epoch 655/1000\n",
      "28956/28956 [==============================] - 38s - loss: 0.0106    \n",
      "Epoch 660/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0106    \n",
      "Epoch 668/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0106    \n",
      "Epoch 683/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0106    \n",
      "Epoch 688/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0106    \n",
      "Epoch 699/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0106    \n",
      "Epoch 715/1000\n",
      "28956/28956 [==============================] - 35s - loss: 0.0106    \n",
      "Epoch 718/1000\n",
      "28956/28956 [==============================] - 37s - loss: 0.0106    \n",
      "Epoch 726/1000\n",
      "28956/28956 [==============================] - 33s - loss: 0.0106    \n",
      "Epoch 740/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 751/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 763/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 766/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 769/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 772/1000\n",
      "28956/28956 [==============================] - 34s - loss: 0.0106    \n",
      "Epoch 775/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 807/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 810/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 813/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 816/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 822/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 859/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 869/1000\n",
      "28956/28956 [==============================] - 32s - loss: 0.0106    \n",
      "Epoch 872/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 875/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 916/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 954/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 957/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 960/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 986/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 989/1000\n",
      "28956/28956 [==============================] - 31s - loss: 0.0106    \n",
      "Epoch 992/1000\n",
      " 8077/28956 [=======>......................] - ETA: 26s - loss: 0.0105"
     ]
    }
   ],
   "source": [
    "encoder.compile(loss = 'mse', optimizer = 'Adadelta')\n",
    "\n",
    "history = encoder.fit(inputs, outputs, nb_epoch = 1000, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder.save_weights(\"W1_pretrain_40.hdf5\")\n",
    "#encoder.load_weights(\"W1_pretrain_Adam_1000_loss_0037.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28956, 300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.output_reconstruction = False\n",
    "encoder.pretrain = False\n",
    "activations = encoder.predict(inputs, batch_size = 15000)\n",
    "#savemat(\"./t1_ntm_pretrain.mat\", { 'activations' : activations,\n",
    "#                                 'W2' : encoder.get_weights()[1]})\n",
    "activations.shape\n",
    "import h5py\n",
    "h5f = h5py.File(\"activations.hdf5\")\n",
    "h5f.create_dataset('activations', data = activations)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 40)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get initial weights for W2 from the autoencoder\n",
    "#pretrained_W2 = encoder.get_weights()[1]\n",
    "#pretrained_W2 = loadmat(to_path + \"t1_ntm_pretrain.mat\", variable_names = \"W2\").get(\"W2\").astype(\"float32\")\n",
    "h5w2 = h5py.File('W1_pretrain_40.hdf5', 'r')\n",
    "h5w2['/layer_2'].items()\n",
    "pretrained_W2 = h5w2['layer_2/param_0'][:]\n",
    "h5w2.close()\n",
    "pretrained_W2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get initial weights for W1 that were pretrained in R based on the autoencoder activations\n",
    "#pretrained_W1 = loadmat(to_path + \"t1_ntm_pret.mat\", variable_names = \"w1\").get(\"w1\").astype(\"float32\") \n",
    "\n",
    "examples = loadmat(to_path + \"t1_ntm_pret.mat\", variable_names = \"examples\").get(\"examples\")\n",
    "# Take the multiple sets and combine them into one big super-epoch\n",
    "examples = numpy.vstack(tuple([examples[:,(0,1,x)] for x in range(2, examples.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954905, 40)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pretrained_W1 = loadmat(to_path + \"t1_ntm_w1.mat\", variable_names = \"w1\").get(\"w1\").astype(\"float32\") \n",
    "h5w1 = h5py.File('w1_pretrain.hdf5', 'r')\n",
    "pretrained_W1 = numpy.transpose(h5w1['w1'][:])\n",
    "h5w1.close()\n",
    "pretrained_W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954905, 40, 28956, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_docs, n_topics, n_terms, n_epochs) = (pretrained_W1.shape[0], \n",
    "                               pretrained_W1.shape[1], \n",
    "                               term_matrix.shape[0], \n",
    "                                        examples.shape[1] - 2)\n",
    "(n_docs, n_topics, n_terms, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DenseNoBias(Layer):\n",
    "    '''Fully connected NN layer with no bias term.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(nb_samples, input_dim)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(nb_samples, output_dim)`.\n",
    "    # Arguments\n",
    "        output_dim: int > 0.\n",
    "        init: name of initialization function for the weights of the layer\n",
    "            (see [initializations](../initializations.md)),\n",
    "            or alternatively, Theano function to use for weights\n",
    "            initialization. This parameter is only relevant\n",
    "            if you don't pass a `weights` argument.\n",
    "        activation: name of activation function to use\n",
    "            (see [activations](../activations.md)),\n",
    "            or alternatively, elementwise Theano function.\n",
    "            If you don't specify anything, no activation is applied\n",
    "            (ie. \"linear\" activation: a(x) = x).\n",
    "        weights: list of numpy arrays to set as initial weights.\n",
    "            The list should have 1 element, of shape `(input_dim, output_dim)`.\n",
    "        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n",
    "            (eg. L1 or L2 regularization), applied to the main weights matrix.\n",
    "        activity_regularizer: instance of [ActivityRegularizer](../regularizers.md),\n",
    "            applied to the network output.\n",
    "        W_constraint: instance of the [constraints](../constraints.md) module\n",
    "            (eg. maxnorm, nonneg), applied to the main weights matrix.\n",
    "        input_dim: dimensionality of the input (integer).\n",
    "            This argument (or alternatively, the keyword argument `input_shape`)\n",
    "            is required when using this layer as the first layer in a model.\n",
    "    '''\n",
    "    input_ndim = 2\n",
    "\n",
    "    def __init__(self, output_dim, init='glorot_uniform', activation='linear', weights=None,\n",
    "                 W_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None, input_dim=None, **kwargs):\n",
    "        self.init = initializations.get(init)\n",
    "        self.activation = activations.get(activation)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.constraints = [self.W_constraint]\n",
    "\n",
    "        self.initial_weights = weights\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape'] = (self.input_dim,)\n",
    "        self.input = K.placeholder(ndim=2)\n",
    "        super(DenseNoBias, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self):\n",
    "        input_dim = self.input_shape[1]\n",
    "\n",
    "        self.W = self.init((input_dim, self.output_dim))\n",
    "\n",
    "        self.params = [self.W]\n",
    "\n",
    "        self.regularizers = []\n",
    "        if self.W_regularizer:\n",
    "            self.W_regularizer.set_param(self.W)\n",
    "            self.regularizers.append(self.W_regularizer)\n",
    "\n",
    "        if self.activity_regularizer:\n",
    "            self.activity_regularizer.set_layer(self)\n",
    "            self.regularizers.append(self.activity_regularizer)\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    @property\n",
    "    def output_shape(self):\n",
    "        return (self.input_shape[0], self.output_dim)\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "        X = self.get_input(train)\n",
    "        output = self.activation(K.dot(X, self.W))\n",
    "        return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'name': self.__class__.__name__,\n",
    "                  'output_dim': self.output_dim,\n",
    "                  'init': self.init.__name__,\n",
    "                  'activation': self.activation.__name__,\n",
    "                  'W_regularizer': self.W_regularizer.get_config() if self.W_regularizer else None,\n",
    "                  'activity_regularizer': self.activity_regularizer.get_config() if self.activity_regularizer else None,\n",
    "                  'W_constraint': self.W_constraint.get_config() if self.W_constraint else None,\n",
    "                  'input_dim': self.input_dim}\n",
    "        base_config = super(DenseNoBias, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the actual training model\n",
    "\n",
    "def build_ntm(term_matrix = term_matrix, \n",
    "              pre_W1 = pretrained_W1, \n",
    "              pre_W2 = pretrained_W2,  \n",
    "              W2_l2 = 0.001\n",
    "             ):\n",
    "    \n",
    "    n_docs = pretrained_W1.shape[0]\n",
    "    n_topics = pretrained_W1.shape[1]\n",
    "    n_terms = term_matrix.shape[0]\n",
    "    \n",
    "    ntm = Graph()\n",
    "    \n",
    "    ntm.add_input(name = \"g\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_node(Embedding(input_dim = n_terms, \n",
    "                          output_dim = 300,\n",
    "                          weights = [term_matrix], \n",
    "                           trainable = False,\n",
    "                           input_length = 1), \n",
    "                 name = \"le\", input = \"g\")\n",
    "    ntm.add_node(Flatten(), input = \"le\", name = \"le_\")\n",
    "    ntm.add_node(DenseNoBias(n_topics, activation = \"sigmoid\", \n",
    "                       weights = [pre_W2], \n",
    "                       W_regularizer = l2(W2_l2)\n",
    "                      ),\n",
    "                 name = \"lt\", input = \"le_\")\n",
    "    \n",
    "    ntm.add_input(name = \"d_pos\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_input(name = \"d_neg\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_shared_node(Embedding(input_dim = n_docs, \n",
    "                                  output_dim = n_topics, \n",
    "                                  weights = [pre_W1], \n",
    "                                  input_length = 1),\n",
    "                        name = \"topicmatrix\",\n",
    "                        inputs =  [\"d_pos\", \"d_neg\"], \n",
    "                        outputs = [\"wd_pos\", \"wd_neg\"],\n",
    "                        merge_mode = None)\n",
    "    ntm.add_node(Flatten(), name = \"wd_pos_\", input = \"wd_pos\")\n",
    "    ntm.add_node(Flatten(), name = \"wd_neg_\", input = \"wd_neg\")\n",
    "    ntm.add_node(Activation(\"softmax\"), name = \"ld_pos\", input = \"wd_pos_\")\n",
    "    ntm.add_node(Activation(\"softmax\"), name = \"ld_neg\", input = \"wd_neg_\")\n",
    "    \n",
    "    ntm.add_node(Layer(),\n",
    "                       name = \"ls_pos\", \n",
    "                       inputs = [\"lt\", \"ld_pos\"], \n",
    "                       merge_mode = 'dot', dot_axes = -1)# , create_output = True)\n",
    "    ntm.add_node(Layer(), \n",
    "                       name = \"ls_neg\", \n",
    "                       inputs = [\"lt\", \"ld_neg\"], \n",
    "                        merge_mode = 'dot', dot_axes = -1)#, create_output = True)\n",
    "    return ntm\n",
    "\n",
    "def add_fine_tuning(ntm = None):\n",
    "    import theano.tensor as T\n",
    "    def output_shape(input_shape):\n",
    "        return (None, 1)\n",
    "    \n",
    "    def sub_merge(layers):\n",
    "        import theano.tensor as T\n",
    "#        ls_pos = T.dot(layers[0], layers[1].T)\n",
    "#        ls_neg = T.dot(layers[0], layers[2].T)\n",
    "        ls_pos = layers[0]\n",
    "        ls_neg = layers[1]\n",
    "        #less = #T.mul(40000000,T.add(ls_neg, ls_pos))\n",
    "        less = T.sub(ls_neg, ls_pos)\n",
    "        return T.add(0.5, less)\n",
    "\n",
    "    #def sumLam(x):\n",
    "    #    return (0.5 + (x[1] - x[0]))\n",
    "\n",
    "    summer = LambdaMerge(layers = [ntm.nodes[\"ls_pos\"], \n",
    "                                   ntm.nodes[\"ls_neg\"]], \n",
    "                     function = sub_merge,\n",
    "                    output_shape = output_shape)\n",
    "    ntm.add_node(summer, inputs = [\"ls_pos\", \"ls_neg\"], name = \"summed\", create_output = True)\n",
    "\n",
    "    return ntm\n",
    "\n",
    "\n",
    "#SVG(to_graph(ntm).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45794004 samples, validate on 934572 samples\n",
      "Epoch 1/20\n",
      "  330000/45794004 [..............................] - ETA: 10931s - loss: 0.6916"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "ntm = build_ntm(W2_l2 = 0.001)\n",
    "ntm = add_fine_tuning(ntm)\n",
    "\n",
    "#def rawloss(x_train, x_test):\n",
    "#    return x_train * x_test\n",
    "def maxloss(y_true, y_predict):\n",
    "    return K.maximum(y_true,y_predict)\n",
    "#    return T.maximum(0., T.mul(y_true,y_predict ))\n",
    "\n",
    "#ntm.load_weights(\"cpw4_starte0_batch10000_sgd001_e_01_0.499998.hdf5\")\n",
    "\n",
    "ntm.compile(loss = {'summed' : maxloss#, \n",
    "                #   'ls_pos' : 'binary_crossentropy', \n",
    "              #     'ls_neg' : 'binary_crossentropy'\n",
    "                   },\n",
    "           optimizer = SGD(lr = 0.01))\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./cpw5_starte0_batch10000_sgd001_e_{epoch:02d}_{val_loss:.6f}.hdf5\", \n",
    "                               monitor = 'val_loss', verbose = 1, save_best_only=False)\n",
    "\n",
    "train_shape = (examples.shape[0], 1)\n",
    "trainer = examples \n",
    "        \n",
    "historylog = ntm.fit(data = {\n",
    "            \"g\" : numpy.reshape(trainer[:,1], train_shape), \n",
    "            \"d_pos\" : numpy.reshape(trainer[:,0], train_shape), \n",
    "            \"d_neg\" : numpy.reshape(trainer[:,2], train_shape),\n",
    "            \"summed\" : numpy.reshape(numpy.zeros(trainer.shape[0], dtype = theano.config.floatX),\n",
    "                                     train_shape)#, \n",
    "#            \"ls_pos\" : numpy.reshape(numpy.ones(trainer.shape[0], dtype = theano.config.floatX),\n",
    "#                                     train_shape),\n",
    "#            \"ls_neg\" : numpy.reshape(numpy.zeros(trainer.shape[0], dtype = theano.config.floatX),\n",
    "#                                     train_shape)\n",
    "        }, callbacks = [checkpointer],\n",
    "        validation_split = 0.02,\n",
    "            nb_epoch = 20, \n",
    "            batch_size = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntm.load_weights(\"cpw_new_startepoch0_00_0.5000.hdf5\")\n",
    "idxs = numpy.random.choice(trainer.shape[0], 200000, replace = False)\n",
    "tester = trainer[idxs,:]\n",
    "tester_shape = (tester.shape[0], 1)\n",
    "ntm.evaluate(data = {\n",
    "            \"g\" : numpy.reshape(tester[:,1], tester_shape), \n",
    "            \"d_pos\" : numpy.reshape(tester[:,0], tester_shape), \n",
    "            \"d_neg\" : numpy.reshape(tester[:,2], tester_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(tester.shape[0], \n",
    "                                                  dtype = theano.config.floatX), tester_shape)\n",
    "        }, batch_size = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.49998338818550109,0.49998418092727659,0.49998493790626525,0.49998548328876496,0.4999860256910324,0.49998660981655119,0.49998704195022581,0.49998756051063536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(x,  type(ntm.nodes[x]), ntm.nodes[x].output_shape) for x in ntm.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = ntm.to_json()\n",
    "open('ntm_final.json', 'w').write(json_string)\n",
    "ntm.save_weights(to_path + 'ntm_finalweights_.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = ntm.get_weights()\n",
    "(weights[0].shape, weights[1].shape, weights[2].shape, \n",
    " weights[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = ntm.nodes[\"lt\"].get_weights()\n",
    "(w[0].shape, w[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "softies = weights[0][100000,:]\n",
    "numpy.exp(softies)/numpy.sum(numpy.exp(softies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1 / (1 + numpy.exp( - weights[2][100,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sNTM\n",
    "def rawloss(x_train, x_test):\n",
    "    return x_train * x_test\n",
    "n_categories = 3\n",
    "ntm.add_node(Dense(n_categories, activation = \"sigmoid\"), input = \"ld_pos\", name = \"ll\")\n",
    "ntm.add_output(name = \"label\", input = \"ll\")\n",
    "ntm.compile(loss = {'loss_out' : threshold,\n",
    "                   'label' : 'categorical_crossentropy'}, \n",
    "           optimizer = \"Adadelta\")\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./cpw_new_smallbatch_sgd001_epoch_{epoch:02d}_{val_loss:.5f}.hdf5\", \n",
    "                               monitor = 'val_loss', verbose = 1, save_best_only=False)\n",
    "\n",
    "train_shape = (examples.shape[0], 1)\n",
    "trainer = examples \n",
    "        \n",
    "historylog = ntm.fit(data = {\n",
    "            \"g\" : numpy.reshape(trainer[:,1], train_shape), \n",
    "            \"d_pos\" : numpy.reshape(trainer[:,0], train_shape), \n",
    "            \"d_neg\" : numpy.reshape(trainer[:,2], train_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape)\n",
    "        # Need to add something here for the labels\n",
    "        }, callbacks = [checkpointer],\n",
    "        validation_split = 0.02,\n",
    "            nb_epoch = 20, \n",
    "            batch_size = 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
