{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy\n",
    "from keras.models import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.embeddings import *\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from keras.models import model_from_json\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import to_graph\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "to_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Matrix of word2vec activations for each term in our dictionary, (n_terms, 300)\n",
    "term_matrix = loadmat(to_path + \"t1_termatrix.mat\", variable_names = \"target\").get(\"target\").astype(\"float32\")\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pretrain W2\n",
    "# W2 is pretrained by autoencoding with the formula le' = sigmoid(le dot W2) dot t(W2)\n",
    "from dA import dA\n",
    "# The dA class from deeplearning.net was modified for this purpose.  \n",
    "# In particular, the changes are:\n",
    "#     1. Biases are taken out. \n",
    "#     2. The visible layer activation function is changed from sigmoid(sigmoid(le dot W2) dot t(W2))\n",
    "#     3. mse is used as the cost function\n",
    "\n",
    "index = T.lscalar()    \n",
    "x = T.matrix('x')\n",
    "rng = numpy.random.RandomState(123)\n",
    "theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "pretrainer = dA(input = x, numpy_rng = rng, \n",
    "                theano_rng = theano_rng, \n",
    "                n_visible = 300, n_hidden = 128)\n",
    "cost, updates = pretrainer.get_cost_updates(\n",
    "        corruption_level=0,\n",
    "        learning_rate=0.01\n",
    "    )\n",
    "train_data = theano.shared(name = \"trainer\", \n",
    "                           value = term_matrix, \n",
    "                           borrow = True)\n",
    "batch_size = 10\n",
    "train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_data[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ultimately, I trained for about 700 epochs\n",
    "# The measure of sparseness was used for validation because\n",
    "# of the risk of exploding/vanishing gradients\n",
    "n_epochs = 1000\n",
    "batch_size = 10\n",
    "batches = term_matrix.shape[0] // batch_size\n",
    "for epoch in xrange(n_epochs):\n",
    "    c = []\n",
    "    for batch_index in xrange(batches):\n",
    "        c.append(train_da(batch_index))\n",
    "    W2_pre = pretrainer.W.get_value(borrow = True)\n",
    "    output = numpy.dot(term_matrix, W2_pre) \n",
    "    output = 1 / (1 + numpy.exp(-output))\n",
    "    sparseness = numpy.sum(output) / (output.shape[0] * output.shape[1])\n",
    "    print 'Training epoch %d, cost %f, sparseness %f' % (epoch, numpy.mean(c), sparseness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The trained weight matrix is extracted, and hidden-unit activations\n",
    "# are extracted and saved to disk for pre-training W1. \n",
    "W2_pre = pretrainer.W.get_value(borrow = True)\n",
    "output = numpy.dot(term_matrix, W2_pre)\n",
    "savemat(\"./t1_ntm_pretrain.mat\", { 'activations' : output,\n",
    "                                 'W2' : W2_pre})\n",
    "(W2_pre.shape, W2_pre[0,:], output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#W2_pre = loadmat(to_path + \"t1_ntm_pretrain.mat\", variable_names = \"W2\").get(\"W2\").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# W1 was pretrained in R\n",
    "# For each document, its pre-trained embedding is the sum of the W1 activations for all terms found in the document\n",
    "pretrained_W1 = loadmat(to_path + \"t1_ntm_pret.mat\", variable_names = \"w1\").get(\"w1\").astype(\"float32\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training set is (n_grams, 2 + n_epochs) matrix\n",
    "# Columns are:\n",
    "#      0. Index of a document (d_pos)\n",
    "#      1. Index of a term found in d_pos (g)\n",
    "#      2...(1 + n_epochs). Indices of randomly selected documents that do not contain g (d_neg)\n",
    "#         d_negs were selected proportionate to the inverse of the number of terms in each document,\n",
    "#         so documents get approximately the same number of total passes in each epoch\n",
    "\n",
    "examples = loadmat(to_path + \"t1_ntm_pret.mat\", variable_names = \"examples\").get(\"examples\")\n",
    "examples = numpy.vstack(tuple([examples[:,(0,1,x)] for x in range(2, examples.shape[1])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(n_docs, n_topics, n_terms, n_total_grams) = (pretrained_W1.shape[0], \n",
    "                               pretrained_W1.shape[1], \n",
    "                               term_matrix.shape[0], \n",
    "                                        examples.shape[0])\n",
    "(n_docs, n_topics, n_terms, n_total_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Basic NTM model\n",
    "def build_ntm(w1_weights, \n",
    "              w2_weights,\n",
    "              term_matrix,\n",
    "              W1_regularizer = l2(0.001), \n",
    "              W2_regularizer = l2(0.001)\n",
    "             ):\n",
    "    \n",
    "    n_docs = w1_weights.shape[0]\n",
    "    n_topics = w2_weights.shape[1]\n",
    "    n_terms = w2_weights.shape[0]\n",
    "    \n",
    "    ntm = Graph()\n",
    "    \n",
    "    ntm.add_input(name = \"d_pos\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_input(name = \"d_neg\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_shared_node(Embedding(input_dim = n_docs, \n",
    "                                  output_dim = n_topics, \n",
    "                                  weights = [w1_weights], \n",
    "                                  W_regularizer = W1_regularizer,\n",
    "                                  input_length = 1),\n",
    "                        name = \"topicmatrix\",\n",
    "                        inputs =  [\"d_pos\", \"d_neg\"], \n",
    "                        outputs = [\"wd_pos\", \"wd_neg\"],\n",
    "                        merge_mode = None)\n",
    "    ntm.add_node(Flatten(), name = \"wd_pos_\", input = \"wd_pos\")\n",
    "    ntm.add_node(Flatten(), name = \"wd_neg_\", input = \"wd_neg\")\n",
    "    ntm.add_node(Activation(\"softmax\"), name = \"ld_pos\", input = \"wd_pos_\")\n",
    "    ntm.add_node(Activation(\"softmax\"), name = \"ld_neg\", input = \"wd_neg_\")\n",
    "    \n",
    "    ntm.add_input(name = \"g\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_node(Embedding(input_dim = n_terms, \n",
    "                          output_dim = 300,\n",
    "                          weights = [term_matrix], \n",
    "                           trainable = False,\n",
    "                           input_length = 1), \n",
    "                 name = \"le\", input = \"g\")\n",
    "    ntm.add_node(Flatten(), input = \"le\", name = \"le_\")\n",
    "    ntm.add_node(Dense(n_topics, activation = \"sigmoid\", \n",
    "                       weights = [w2_weights, numpy.zeros(n_topics)], \n",
    "                       W_regularizer = W2_regularizer),\n",
    "                 name = \"lt\", input = \"le_\")\n",
    "    \n",
    "    ntm.add_node(Layer(),\n",
    "                       name = \"ls_pos\", \n",
    "                       inputs = [\"lt\", \"ld_pos\"], \n",
    "                       merge_mode = 'dot', dot_axes = -1)\n",
    "    ntm.add_node(Layer(), \n",
    "                       name = \"ls_neg\", \n",
    "                       inputs = [\"lt\", \"ld_neg\"], \n",
    "                        merge_mode = 'dot', dot_axes = -1)\n",
    "    return ntm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Very large batch sizes are a good idea.  \n",
    "# Even with very large batches, the its unlikely that many weights in W1 will be \n",
    "# triggered more than once each batch.  But, W2 weights get updated from every row. \n",
    "# W2 therefore wants to overfit before W1 is finished training. Using large batch\n",
    "# sizes mitigates the effect. \n",
    "batch_size = 20000\n",
    "n_epochs = 10\n",
    "margin = 0.5\n",
    "ntm = build_ntm(\n",
    "              w1_weights = pretrained_W1, \n",
    "              w2_weights = W2_pre,\n",
    "              term_matrix = term_matrix,\n",
    "              W1_regularizer = l2(0.001), \n",
    "              W2_regularizer = l2(0.001))\n",
    "\n",
    "def output_shape(input_shape):\n",
    "    return (None, 1)\n",
    "\n",
    "def sumLam(x):\n",
    "    return (margin + (x[1] - x[0]))\n",
    "\n",
    "summer = LambdaMerge(layers = [ntm.nodes[\"ls_pos\"], ntm.nodes[\"ls_neg\"] ], \n",
    "                     function = sumLam,\n",
    "                     output_shape = output_shape)\n",
    "ntm.add_node(summer, inputs = [\"ls_pos\", \"ls_neg\"], \n",
    "             name = \"summed\")\n",
    "ntm.add_output(name = \"loss_out\",  input= \"summed\")\n",
    "\n",
    "def rawloss(x_train, x_test):\n",
    "    return x_train * x_test\n",
    "\n",
    "# Adadelta tended to converge more quickly than SGD\n",
    "ntm.compile(loss = {'loss_out' : rawloss},\n",
    "           optimizer = 'Adadelta') \n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./checkpointweights.hdf5\", verbose = 1, save_best_only=True)\n",
    "\n",
    "train_data = examples\n",
    "train_shape = (train_data.shape[0], 1)\n",
    "g = numpy.reshape(examples[:,1], train_shape)\n",
    "d_pos = numpy.reshape(examples[:,0], train_shape)\n",
    "d_neg = numpy.reshape(examples[:,2], train_shape)\n",
    "        \n",
    "ntm.fit(data = {\n",
    "            \"g\" : g, \n",
    "            \"d_pos\" : d_pos, \n",
    "            \"d_neg\" : d_neg,\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape)\n",
    "        }, callbacks = [checkpointer],\n",
    "        validation_split = 0.005,\n",
    "        nb_epoch = n_epochs, \n",
    "        batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_string = ntm.to_json()\n",
    "open('ntm_final.json', 'w').write(json_string)\n",
    "ntm.save_weights(to_path + 'ntm_finalweights_.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sNTM - Not fully tested\n",
    "n_categories = 3\n",
    "ntm.add_node(Dense(n_categories, activation = \"sigmoid\"), input = \"ld_pos\", name = \"ll\")\n",
    "ntm.add_output(name = \"label\", input = \"ll\")\n",
    "ntm.compile(loss = {'loss_out' : threshold,\n",
    "                   'label' : 'categorical_crossentropy'}, \n",
    "           optimizer = \"Adadelta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
