{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1 -- pretrain a matrix of the embedding of each term, out_dims = n_topics\n",
    "#    1.a.  Cost function is sum(abs(h(x) - (sigmoid(x * W2) * W2t)))\n",
    "#2 -- Create W1, which is (n_docs * n_topics).  W1[doc,:] = normalize(sum(embeddings of words in doc))\n",
    "#3 -- Train\n",
    "#    3.a.  The training examples are all term-document combinations. \n",
    "#    3.b   For each training example (term, doc), select a random document that does not contain the term\n",
    "#    3.c   Minimize \n",
    "# Note -- can pass a regularizer function, zero_grad(g, p): return 0., to a layer to prevent it from being trained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import numpy\n",
    "from keras.models import *\n",
    "from keras.layers.core import *\n",
    "from keras.layers.embeddings import *\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l2, l1l2\n",
    "from keras.constraints import unitnorm,nonneg\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.layers.advanced_activations import ThresholdedReLU\n",
    "from keras import backend as K\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from keras.models import model_from_json\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import to_graph\n",
    "from keras.callbacks import ModelCheckpoint,RemoteMonitor\n",
    "import theano\n",
    "to_path = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28956, 300)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix = loadmat(to_path + \"t1_termatrix.mat\", variable_names = \"target\").get(\"target\").astype(\"float32\")\n",
    "term_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pretrain W2, the distribution of topics over grams\n",
    "n_topics = 128\n",
    "\n",
    "encoder = Sequential([Dense(n_topics, input_dim=300,\n",
    "                           W_regularizer = l1l2(l1 = 0.0001, l2 = .001), \n",
    "                            b_regularizer = l1l2(l1 = 0.0001, l2 = 0.001),\n",
    "                            activation = \"sigmoid\")])\n",
    "decoder = Sequential([Dense(300, input_dim=n_topics)])\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(AutoEncoder(encoder = encoder, \n",
    "                            decoder = decoder, \n",
    "                            output_reconstruction=False))\n",
    "\n",
    "autoencoder.compile(loss = \"mse\", optimizer=Adadelta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = autoencoder.fit(term_matrix, term_matrix, batch_size = 1, nb_epoch = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "json_string = autoencoder.to_json()\n",
    "open('autoencoder.json', 'w').write(json_string)\n",
    "autoencoder.save_weights('autoencoder.h5', overwrite=True)\n",
    "#save activations for use in pretraining\n",
    "output = autoencoder.predict(term_matrix)\n",
    "savemat(\"./t1_ntm_pretrain.mat\", { 'activations' : output})\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load model\n",
    "#autoencoder = model_from_json(open('autoencoder.json').read())\n",
    "#autoencoder.load_weights('autoencoder.h5')\n",
    "#pretrained_W2 = autoencoder.layers[0].encoder.get_weights()\n",
    "#numpy.save(file = \"t1_w2_pretrain.npy\", arr = pretrained_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get initial weights for W2 from the autoencoder\n",
    "pretrained_W2 = numpy.load(to_path + \"t1_w2_pretrain.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get initial weights for W1 that were pretrained in R based on the autoencoder activations\n",
    "pretrained_W1 = loadmat(to_path + \"t1_ntm_pret.mat\", variable_names = \"w1\").get(\"w1\").astype(\"float32\")\n",
    "#get training examples\n",
    "examples = loadmat(to_path + \"t1_ntm_pret.mat\", variable_names = \"examples\").get(\"examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(954905, 128, 28956, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(n_docs, n_topics, n_terms, n_epochs) = (pretrained_W1.shape[0], \n",
    "                               pretrained_W1.shape[1], \n",
    "                               term_matrix.shape[0], \n",
    "                                        examples.shape[1] - 2)\n",
    "(n_docs, n_topics, n_terms, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the actual training model\n",
    "def build_ntm():\n",
    "    ntm = Graph()\n",
    "    ntm.add_input(name = \"d_pos\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_input(name = \"d_neg\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_shared_node(Embedding(input_dim = n_docs, \n",
    "                                  output_dim = n_topics, \n",
    "                                  weights = [pretrained_W1], \n",
    "    #                              W_regularizer = l2(0.001),\n",
    "                                  W_regularizer = l2(0.05), \n",
    "                                  input_length = 1),\n",
    "                        name = \"topicmatrix\",\n",
    "                        inputs =  [\"d_pos\", \"d_neg\"], \n",
    "                        outputs = [\"wd_pos\", \"wd_neg\"],\n",
    "                        merge_mode = None)\n",
    "    ntm.add_input(name = \"g\", input_shape = (1,), dtype = \"int\")\n",
    "    ntm.add_node(Embedding(input_dim = n_terms, \n",
    "                          output_dim = 300,\n",
    "                          weights = [term_matrix], \n",
    "                           trainable = False,\n",
    "                           input_length = 1), \n",
    "                 name = \"le\", input = \"g\")\n",
    "    ntm.add_node(Flatten(), input = \"le\", name = \"le_\")\n",
    "    ntm.add_node(Dense(n_topics, activation = \"sigmoid\", \n",
    "                       weights = pretrained_W2, \n",
    "                 #      W_regularizer = l2(0.001)),\n",
    "                         W_regularizer = l2(0.05)),\n",
    "                 name = \"lt\", input = \"le_\")\n",
    "    ntm.add_node(Flatten(), name = \"wd_pos_\", input = \"wd_pos\")\n",
    "    ntm.add_node(Flatten(), name = \"wd_neg_\", input = \"wd_neg\")\n",
    "    ntm.add_node(Activation(\"softmax\"), name = \"ld_pos\", input = \"wd_pos_\")\n",
    "    #ntm.add_node(Reshape((1,128)), input = \"ld_pos\", name = \"ld_pos_\")\n",
    "    ntm.add_node(Activation(\"softmax\"), name = \"ld_neg\", input = \"wd_neg_\")\n",
    "    #ntm.add_node(Reshape((1,128)), input = \"ld_neg\", name = \"ld_neg_\")\n",
    "    ls_pos = Layer()\n",
    "    ntm.add_node(ls_pos,\n",
    "                       name = \"ls_pos\", \n",
    "                       inputs = [\"lt\", \"ld_pos\"], \n",
    "                       merge_mode = 'dot', dot_axes = -1)\n",
    "    ls_neg = Layer()\n",
    "    ntm.add_node(ls_neg, \n",
    "                       name = \"ls_neg\", \n",
    "                       inputs = [\"lt\", \"ld_neg\"], \n",
    "                        merge_mode = 'dot', dot_axes = -1)\n",
    "    return ntm\n",
    "\n",
    "#SVG(to_graph(ntm).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Pre-training\n",
    "ntm = build_ntm()\n",
    "ntm.add_output(\"pos\", input = \"ls_pos\")\n",
    "ntm.add_output(\"neg\", input = \"ls_neg\")\n",
    "\n",
    "ntm.compile(loss = {'pos' : \"binary_crossentropy\",\n",
    "                   'neg' : 'binary_crossentropy'}, \n",
    "           optimizer = \"Adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  105000/11682144 [..............................] - ETA: 3688s - loss: 1.3863"
     ]
    }
   ],
   "source": [
    "# In this pre-training stage, try to target pos toward 1 and neg toward 0.\n",
    "# The goal is to prevent the topics from all distributing evenly, which causes\n",
    "# the losses to underflow to zero. \n",
    "\n",
    "trainer = examples#[0:100000,:]\n",
    "train_shape = (trainer.shape[0],1)\n",
    "        \n",
    "ntm.fit(data = {\n",
    "            \"g\" : numpy.reshape(trainer[:,1], train_shape), \n",
    "            \"d_pos\" : numpy.reshape(trainer[:,0], train_shape), \n",
    "            \"d_neg\" : numpy.reshape(trainer[:,trainer.shape[1]-2], train_shape),\n",
    "            \"pos\" : numpy.reshape(numpy.ones(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape),\n",
    "            \"neg\" : numpy.reshape(numpy.zeros(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape)\n",
    "        },\n",
    "            nb_epoch = 1, \n",
    "            batch_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fine-tuning\n",
    "ntm2 = build_ntm()\n",
    "ntm2.set_weights(ntm.get_weights())\n",
    "\n",
    "def output_shape(input_shape):\n",
    "    return (None, 1)\n",
    "\n",
    "def sumLam(x):\n",
    "    return (x[1] - x[0])\n",
    "summer = LambdaMerge(layers = [ntm.nodes[\"ls_pos\"], \n",
    "                               ntm.nodes[\"ls_neg\"]], \n",
    "                     function = sumLam,\n",
    "                    output_shape = output_shape)\n",
    "ntm.add_node(summer, inputs = [\"ls_pos\", \"ls_neg\"], name = \"summed\")\n",
    "\n",
    "ntm.add_input(name = \"gradient_enhancer\", input_shape = (1,), dtype = \"float\")\n",
    "ntm.add_node(Layer(), name = \"enhanced\", \n",
    "             inputs = [\"gradient_enhancer\", \"summed\"],\n",
    "            merge_mode = \"mul\")\n",
    "\n",
    "ntm.add_node(ThresholdedReLU(0.5), input = \"enhanced\", name = \"threshold\")\n",
    "ntm.add_output(name = \"loss_out\",  input= \"threshold\")\n",
    "\n",
    "ntm.compile(loss = {'loss_out' : \"rmse\"},\n",
    "           optimizer = \"Adadelta\")\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./checkpointweights.hdf5\", verbose = 1, save_best_only=True)\n",
    "from keras.callbacks import Callback\n",
    "class CheckFit(Callback):\n",
    "    def on_epoch_end(self, logs = {}):\n",
    "        print logs.keys()\n",
    "\n",
    "checker = CheckFit()\n",
    "\n",
    "remote = RemoteMonitor(root=\"http://localhost:9000\")\n",
    "\n",
    "validation_column = examples.shape[1] - 1\n",
    "validation_length = 20000\n",
    "validation_indices = numpy.random.choice(examples.shape[0], \n",
    "                                  size = validation_length, \n",
    "                                  replace = False)\n",
    "validation_shape = (validation_length,1)\n",
    "\n",
    "trainer = examples#[0:100000,:]\n",
    "train_shape = (trainer.shape[0],1)\n",
    "        \n",
    "for epoch in xrange(3, trainer.shape[1] - 3):\n",
    "    ntm.fit(data = {\n",
    "            \"g\" : numpy.reshape(trainer[:,1], train_shape), \n",
    "            \"d_pos\" : numpy.reshape(trainer[:,0], train_shape), \n",
    "            \"d_neg\" : numpy.reshape(trainer[:,epoch + 2], train_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.zeros(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape),\n",
    "            \"gradient_enhancer\" : numpy.reshape(numpy.repeat(max(1., n_docs / (10 ** epoch)), trainer.shape[0], 0), \n",
    "                                                train_shape)\n",
    "        }, callbacks = [checkpointer, remote], \n",
    "            validation_data = {\n",
    "            \"g\" : numpy.reshape(examples[validation_indices,1], \n",
    "                                validation_shape), \n",
    "            \"d_pos\" : numpy.reshape(examples[validation_indices,0], \n",
    "                                    validation_shape), \n",
    "            \"d_neg\" : numpy.reshape(examples[validation_indices,validation_column], \n",
    "                                    validation_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(validation_shape[0], \n",
    "                         dtype = theano.config.floatX), validation_shape),\n",
    "            \"gradient_enhancer\" : numpy.reshape(numpy.repeat(n_docs, validation_shape[0], 0), \n",
    "                                                validation_shape)\n",
    "        },\n",
    "            nb_epoch = 1, \n",
    "            batch_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune\n",
    "checkpointer = ModelCheckpoint(filepath=\"./checkpointweights.hdf5\", verbose = 1, save_best_only=True)\n",
    "from keras.callbacks import Callback\n",
    "import theano\n",
    "class CheckFit(Callback):\n",
    "    def on_epoch_end(self, logs = {}):\n",
    "        print logs.keys()\n",
    "\n",
    "checker = CheckFit()\n",
    "\n",
    "remote = RemoteMonitor(root=\"http://localhost:9000\")\n",
    "\n",
    "validation_column = examples.shape[1] - 1\n",
    "validation_length = 20000\n",
    "validation_indices = numpy.random.choice(examples.shape[0], \n",
    "                                  size = validation_length, \n",
    "                                  replace = False)\n",
    "validation_shape = (validation_length,1)\n",
    "\n",
    "trainer = examples#[0:100000,:]\n",
    "train_shape = (trainer.shape[0],1)\n",
    "        \n",
    "for epoch in xrange(trainer.shape[1] - 3):\n",
    "    ntm.fit(data = {\n",
    "            \"g\" : numpy.reshape(trainer[:,1], train_shape), \n",
    "            \"d_pos\" : numpy.reshape(trainer[:,0], train_shape), \n",
    "            \"d_neg\" : numpy.reshape(trainer[:,epoch + 2], train_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape),\n",
    "            \"gradient_enhancer\" : numpy.reshape(numpy.repeat(max(1., n_docs / (10 ** epoch)), trainer.shape[0], 0), \n",
    "                                                train_shape)\n",
    "        }, callbacks = [checkpointer, remote], \n",
    "            validation_data = {\n",
    "            \"g\" : numpy.reshape(examples[validation_indices,1], \n",
    "                                validation_shape), \n",
    "            \"d_pos\" : numpy.reshape(examples[validation_indices,0], \n",
    "                                    validation_shape), \n",
    "            \"d_neg\" : numpy.reshape(examples[validation_indices,validation_column], \n",
    "                                    validation_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(validation_shape[0], \n",
    "                         dtype = theano.config.floatX), validation_shape),\n",
    "            \"gradient_enhancer\" : numpy.reshape(numpy.repeat(n_docs, validation_shape[0], 0), \n",
    "                                                validation_shape)\n",
    "        },\n",
    "            nb_epoch = 1, \n",
    "            batch_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[(x,  type(ntm.nodes[x]), ntm.nodes[x].output_shape) for x in ntm.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug\n",
    "ntm.add_node(Layer(), name = \"out_monitor\", \n",
    "            inputs = [\"ls_pos\", \"ls_pos_neg\", \"ls_neg\", \"summed\", \"enhanced\", \"threshold\"],\n",
    "            merge_mode=\"concat\")\n",
    "ntm.add_node(Layer(), name = \"mult_monitor\", inputs = [\"lt\", \"ld_neg\", \"ld_pos\"], \n",
    "            merge_mode=\"concat\", concat_axis = 0)\n",
    "ntm.add_output(name = \"multmonitor\", input = \"mult_monitor\")\n",
    "ntm.add_output(name = \"monitor\", input = \"out_monitor\")\n",
    "\n",
    "\n",
    "ntm.add_output(name = \"pos_out\", input = \"ls_pos\")\n",
    "ntm.add_output(name = \"neg_out\", input = \"ls_neg\")\n",
    "import theano\n",
    "get_monitor = theano.function([ntm.inputs[i].input for i in ntm.input_order],\n",
    "                                        [ntm.outputs['monitor'].get_output(train=False),\n",
    "                                         ntm.outputs['multmonitor'].get_output(train=False)],\n",
    "                                        on_unused_input='ignore',\n",
    "                                        allow_input_downcast=True)\n",
    "tod = get_monitor([[125418]], \n",
    "            [[29141]], \n",
    "            [[0]], \n",
    "            [[10000000]])\n",
    "(tod[0], tod[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.95)\n",
    "def threshold(y_true, y_pred):\n",
    "    return K.maximum(y_true * y_pred, 0.)#, axis=1\n",
    "\n",
    "ntm.compile(loss = {'loss_out' : threshold},\n",
    "#                   'monitor' : 'mse'}, \n",
    "           optimizer = \"Adadelta\")\n",
    "#ntm.compile(loss = {'pos_out' : \"hinge\", \n",
    "#                   'neg_out' : neg_hinge}, \n",
    "#            optimizer = sgd)#) \"Adadelta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A large batch size does not have a detrimental effect on training W1\n",
    "# because with 1 million rows, its unlikely two will be active in a single batch anyway. \n",
    "# So the pattern will be to train an epoch with a batch size of 100, for W2, \n",
    "# with the loss functions set so that ls_pos chases 1 and ls_neg chases 0.  \n",
    "# Then, to fine-tune W1, we'll take the same weights, use the 0.5 separation recommended in\n",
    "# the paper, and train several epochs with as large a batch size as the GPU can hold.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"./checkpointweights.hdf5\", verbose = 1, save_best_only=True)\n",
    "from keras.callbacks import Callback\n",
    "import theano\n",
    "class CheckFit(Callback):\n",
    "    def on_epoch_end(self, logs = {}):\n",
    "        print logs.keys()\n",
    "\n",
    "checker = CheckFit()\n",
    "\n",
    "remote = RemoteMonitor(root=\"http://localhost:9000\")\n",
    "\n",
    "validation_column = examples.shape[1] - 1\n",
    "validation_length = 20000\n",
    "validation_indices = numpy.random.choice(examples.shape[0], \n",
    "                                  size = validation_length, \n",
    "                                  replace = False)\n",
    "validation_shape = (validation_length,1)\n",
    "\n",
    "trainer = examples#[0:100000,:]\n",
    "train_shape = (trainer.shape[0],1)\n",
    "        \n",
    "for epoch in xrange(3, trainer.shape[1] - 3):\n",
    "    ntm.fit(data = {\n",
    "            \"g\" : numpy.reshape(trainer[:,1], train_shape), \n",
    "            \"d_pos\" : numpy.reshape(trainer[:,0], train_shape), \n",
    "            \"d_neg\" : numpy.reshape(trainer[:,epoch + 2], train_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(trainer.shape[0], \n",
    "                                                  dtype = theano.config.floatX), train_shape),\n",
    "            \"gradient_enhancer\" : numpy.reshape(numpy.repeat(max(1., n_docs / (10 ** epoch)), trainer.shape[0], 0), \n",
    "                                                train_shape)\n",
    "        }, callbacks = [checkpointer, remote], \n",
    "            validation_data = {\n",
    "            \"g\" : numpy.reshape(examples[validation_indices,1], \n",
    "                                validation_shape), \n",
    "            \"d_pos\" : numpy.reshape(examples[validation_indices,0], \n",
    "                                    validation_shape), \n",
    "            \"d_neg\" : numpy.reshape(examples[validation_indices,validation_column], \n",
    "                                    validation_shape),\n",
    "            \"loss_out\" : numpy.reshape(numpy.ones(validation_shape[0], \n",
    "                         dtype = theano.config.floatX), validation_shape),\n",
    "            \"gradient_enhancer\" : numpy.reshape(numpy.repeat(n_docs, validation_shape[0], 0), \n",
    "                                                validation_shape)\n",
    "        },\n",
    "            nb_epoch = 1, \n",
    "            batch_size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntm.load_weights('ntm2.h5')\n",
    "start = 6000\n",
    "end = 10000\n",
    "length = end - start\n",
    "loss = ntm.test_on_batch(data = {\n",
    "            \"g\" : numpy.reshape(examples[start:end,1], (length,1)), \n",
    "            \"d_pos\" : numpy.reshape(examples[start:end,0], (length,1)), \n",
    "            \"d_neg\" : numpy.reshape(examples[start:end,2], (length,1)),\n",
    "            \"pos_out\" : numpy.reshape(numpy.ones(length), (length,1)),\n",
    "            \"neg_out\" : numpy.reshape(numpy.zeros(length), (length,1))\n",
    "        })\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#json_string = ntm.to_json()ntm.save_weights(to_path + 'ntm2' + str(epoch) + '.h5', overwrite=True)\n",
    "#open('ntm.json', 'w').write(json_string)\n",
    "ntm.save_weights(to_path + 'ntm4adadelta_' + str(1) + 'epoch.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ntm.fit(data = {\n",
    "#        \"g\" : numpy.reshape(examples[:,1], (examples.shape[0],1)), \n",
    "#        \"d_pos\" : numpy.reshape(examples[:,0], (examples.shape[0],1)), \n",
    "#        \"d_neg\" : numpy.reshape(examples[:,2], (examples.shape[0],1)),\n",
    "#        \"loss_out\" : numpy.reshape(numpy.ones(examples.shape[0]), (examples.shape[0],1))\n",
    "#    }, \n",
    "#        nb_epoch = 4, \n",
    "#        batch_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((954905, 128), (28956, 300), (300, 128), (128,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = ntm.get_weights()\n",
    "(weights[0].shape, weights[1].shape, weights[2].shape, \n",
    " weights[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.sum(weights[0][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -5.05579978e-09,  -2.03678163e-09,  -4.53540050e-09,\n",
       "        -6.96222413e-10,  -1.03804121e-09,   1.40375767e-09,\n",
       "         2.78581869e-09,   4.54158655e-10,   2.89217295e-09,\n",
       "         1.46871804e-09,   1.61418123e-09,   8.70731209e-10,\n",
       "        -3.20436677e-09,   6.64569622e-10,  -4.49658533e-09,\n",
       "         3.06334647e-09,   7.66106956e-10,   3.56550900e-09,\n",
       "        -3.38904149e-09,   2.63906030e-09,   1.32899647e-09,\n",
       "        -2.94247093e-09,  -3.19365578e-09,  -4.35888756e-11,\n",
       "         7.52533924e-10,  -6.76807221e-10,   2.06828354e-09,\n",
       "        -9.62975588e-10,   1.37461112e-10,  -5.16154763e-09,\n",
       "         2.07746664e-09,   8.62301008e-10,  -5.29927879e-09,\n",
       "        -7.00579372e-10,  -2.78500689e-09,   1.58791402e-09,\n",
       "         9.98281235e-10,   2.54655741e-09,  -3.02575032e-09,\n",
       "         1.83062887e-09,   1.87147653e-09,   1.40025525e-09,\n",
       "        -3.48459661e-09,  -3.28445204e-09,   6.48316345e-10,\n",
       "         1.35633560e-09,   1.36060108e-09,  -1.29507927e-09,\n",
       "         8.37023006e-10,   1.66193470e-09,   1.82358373e-09,\n",
       "        -4.84758567e-10,   8.09394940e-10,   1.46467471e-09,\n",
       "         2.53592236e-09,  -8.58749960e-10,   3.61067287e-10,\n",
       "         2.46945020e-09,   2.63123234e-09,   1.37009948e-09,\n",
       "         1.94310878e-09,   4.51013366e-10,  -3.04341108e-09,\n",
       "         1.76666781e-09,  -3.41014189e-10,   2.28409425e-09,\n",
       "         2.70511924e-09,  -1.61836294e-10,   1.15328536e-09,\n",
       "         1.60995428e-09,  -4.85320095e-09,   6.95870361e-10,\n",
       "        -5.52957635e-09,   6.41950382e-10,  -1.38004697e-09,\n",
       "        -4.10587830e-09,   2.17217688e-09,   1.62270586e-09,\n",
       "        -3.45970408e-09,   1.24075944e-10,  -7.94126598e-10,\n",
       "         2.86565971e-09,  -7.73361042e-10,   2.19261254e-09,\n",
       "         2.74325918e-09,  -3.04935899e-09,   1.85224636e-09,\n",
       "         1.68221881e-09,  -1.50691937e-09,   4.87217877e-10,\n",
       "        -2.24230923e-09,  -1.58271274e-09,   1.74076775e-09,\n",
       "        -1.25965494e-09,  -3.57706664e-09,  -4.34935243e-09,\n",
       "         5.94952032e-10,  -7.56053109e-10,   9.53015777e-10,\n",
       "        -1.85061508e-10,   2.13151985e-09,   4.69290329e-10,\n",
       "        -5.05534448e-10,   2.63426947e-09,   2.31496822e-09,\n",
       "        -3.35879435e-10,  -2.74655498e-09,  -3.94480049e-09,\n",
       "         9.74460956e-10,  -4.36592901e-10,   2.40754727e-09,\n",
       "        -1.28579403e-09,   6.73198830e-10,   4.37050091e-10,\n",
       "         9.51356882e-10,   7.06667669e-10,   2.44102427e-09,\n",
       "         1.25727384e-09,   1.25108390e-09,   1.79590887e-09,\n",
       "         8.07224010e-10,  -1.86828553e-09,   1.37525802e-09,\n",
       "         2.23148211e-09,  -4.82065987e-09,   2.56591104e-09,\n",
       "        -2.05147321e-09,   9.10367670e-10], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softies = weights[0][100000,:]\n",
    "numpy.exp(softies)/numpy.sum(numpy.exp(softies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights[0][0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntm.nodes.get(\"wd_pos\").output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sNTM\n",
    "n_categories = 3\n",
    "ntm.add_node(Dense(n_categories, activation = \"sigmoid\"), input = \"ld_pos\", name = \"ll\")\n",
    "ntm.add_output(name = \"label\", input = \"ll\")\n",
    "ntm.compile(loss = {'loss_out' : threshold,\n",
    "                   'label' : 'categorical_crossentropy'}, \n",
    "           optimizer = \"Adadelta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
